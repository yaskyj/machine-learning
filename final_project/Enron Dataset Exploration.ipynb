{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "features_list = ['poi','salary'] # You will need to use more features\n",
    "### Features selection will be done after some removal of errant values, na values filled, and features creation\n",
    "### has occurred\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "### Create a dataframe from the dictionary using pandas\n",
    "df = pd.DataFrame.from_dict(data_dict, orient='index').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Convert all columns besides email_address and poi to numbers\n",
    "cols = [c for c in df.columns if c not in ['index', 'poi', 'email_address']]\n",
    "df[cols] = df[cols].apply(pd.to_numeric, errors='coerce', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number people in dataset: 146\n",
      "Number of POI: 18\n",
      "Number of Non POI: 128\n",
      "Number of features in data: 22\n"
     ]
    }
   ],
   "source": [
    "### Some basic data exploration\n",
    "poi = df[df.poi == True]\n",
    "non_poi = df[df.poi == False]\n",
    "print \"Total number people in dataset: %.0f\\nNumber of POI: %.0f\\nNumber of Non POI: %.0f\\nNumber of features in data: %.0f\" % (len(df), \n",
    "      len(poi), len(non_poi), len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totals NAs for each number column:\n",
      "index                          0\n",
      "salary                        51\n",
      "to_messages                   60\n",
      "deferral_payments            107\n",
      "total_payments                21\n",
      "exercised_stock_options       44\n",
      "bonus                         64\n",
      "restricted_stock              36\n",
      "shared_receipt_with_poi       60\n",
      "restricted_stock_deferred    128\n",
      "total_stock_value             20\n",
      "expenses                      51\n",
      "loan_advances                142\n",
      "from_messages                 60\n",
      "other                         53\n",
      "from_this_person_to_poi       60\n",
      "poi                            0\n",
      "director_fees                129\n",
      "deferred_income               97\n",
      "long_term_incentive           80\n",
      "email_address                  0\n",
      "from_poi_to_this_person       60\n",
      "dtype: int64 \n",
      "Total NAs for email address: 35\n"
     ]
    }
   ],
   "source": [
    "### Total NaN values for each column \n",
    "print \"Totals NAs for each number column:\\n\", df.isnull().sum(), \"\\nTotal NAs for email address: %.0f\" % (len(df[df['email_address'] == 'NaN']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 2: Remove outliers\n",
    "### Upon inspecting the dataset, the TOTAL column was added. This is removed below.\n",
    "df = df[df['index'] != 'TOTAL']\n",
    "df = df[df['index'] != 'THE TRAVEL AGENCY IN THE PARK']\n",
    "df = df[df['index'] != 'LOCKHART EUGENE E']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Now we'll fill NaN amounts with 0\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 3: Create new feature(s)\n",
    "### New features using all emails including poi with this person\n",
    "df['expense_percent'] = df['expenses']/(df['salary']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Temporary features for analysis\n",
    "temp_features = [c for c in df.columns if c not in ['index', 'poi', 'email_address']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### First we'll scale the features. We will probably be trying out SVM so, while not necessary for all classifers\n",
    "### it won't adversely impact, for example, Random Forests, while SVM will be negatively impacted by not feature\n",
    "### scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[temp_features] = scaler.fit_transform(df[temp_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salary 3.05278674479\n",
      "to_messages 0.436397768802\n",
      "deferral_payments 0.0606966069314\n",
      "total_payments 2.78477883965\n",
      "exercised_stock_options 6.84550933503\n",
      "bonus 5.12075413709\n",
      "restricted_stock 0.589535349487\n",
      "shared_receipt_with_poi 2.43221986514\n",
      "restricted_stock_deferred 0.00350676503321\n",
      "total_stock_value 5.47661009929\n",
      "expenses 1.48610336666\n",
      "loan_advances 6.68878173834\n",
      "from_messages 0.0687385421513\n",
      "other 1.7159505308\n",
      "from_this_person_to_poi 1.0008076418\n",
      "director_fees 1.50113085359\n",
      "deferred_income 0.340099218406\n",
      "long_term_incentive 2.53848503308\n",
      "from_poi_to_this_person 1.37005929223\n",
      "expense_percent 0.00123674763195\n"
     ]
    }
   ],
   "source": [
    "### Take the top half of the features selected with RFECV\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "selector = SelectPercentile(chi2, percentile=100).fit(df[temp_features], df['poi'])\n",
    "feat_scores = [(a,t) for a, t, z in zip(temp_features, selector.scores_, selector.get_support()) if z] \n",
    "for i in  feat_scores: print i[0], i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Take the features selected by SelectKBest\n",
    "temp_features = [a for a, t in zip(temp_features, selector.get_support()) if t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "# Importing a variety of classifiers\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=3, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_features = ['bonus', 'exercised_stock_options', 'total_stock_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics_test = {\"SVM\":{\"precision\":[], \"recall\":[]}, \"Forest\":{\"precision\":[], \"recall\":[]}, \"Naive\":{\"precision\":[], \"recall\":[]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "0.0\n",
      "0.0\n",
      "SVM\n",
      "0.0\n",
      "0.0\n",
      "SVM\n",
      "0.5\n",
      "0.2\n",
      "Random Forest\n",
      "0.0\n",
      "0.0\n",
      "Random Forest\n",
      "0.5\n",
      "0.2\n",
      "Random Forest\n",
      "1.0\n",
      "0.2\n",
      "Naive Bayes\n",
      "0.185185185185\n",
      "1.0\n",
      "Naive Bayes\n",
      "0.107142857143\n",
      "0.6\n",
      "Naive Bayes\n",
      "0.129032258065\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in sss.split(df[temp_features], df['poi']):\n",
    "    X_train, X_test = df[temp_features].iloc[train_index], df[temp_features].iloc[test_index]\n",
    "    y_train, y_test = df['poi'].iloc[train_index], df['poi'].iloc[test_index]\n",
    "\n",
    "    print \"SVM\"\n",
    "    svm = SVC(kernel='linear', C=10)\n",
    "    svm = svm.fit(X_train, y_train)\n",
    "    pred = svm.predict(X_test)\n",
    "    print precision_score(y_test, pred)\n",
    "    print recall_score(y_test, pred)\n",
    "    metrics_test['SVM']['precision'].append(precision_score(y_test, pred))\n",
    "    metrics_test['SVM']['recall'].append(recall_score(y_test, pred))\n",
    "\n",
    "for train_index, test_index in sss.split(df[temp_features], df['poi']):\n",
    "    X_train, X_test = df[temp_features].iloc[train_index], df[temp_features].iloc[test_index]\n",
    "    y_train, y_test = df['poi'].iloc[train_index], df['poi'].iloc[test_index]\n",
    "\n",
    "    print \"Random Forest\"\n",
    "    forest = RandomForestClassifier(min_samples_split=5, n_estimators=100)\n",
    "    forest = forest.fit(X_train, y_train)\n",
    "    pred = forest.predict(X_test)\n",
    "    print precision_score(y_test, pred)\n",
    "    print recall_score(y_test, pred)\n",
    "    metrics_test['Forest']['precision'].append(precision_score(y_test, pred))\n",
    "    metrics_test['Forest']['recall'].append(recall_score(y_test, pred))\n",
    "\n",
    "for train_index, test_index in sss.split(df[temp_features], df['poi']):\n",
    "    X_train, X_test = df[temp_features].iloc[train_index], df[temp_features].iloc[test_index]\n",
    "    y_train, y_test = df['poi'].iloc[train_index], df['poi'].iloc[test_index]\n",
    "\n",
    "    print \"Naive Bayes\"\n",
    "    clf = GaussianNB()\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "    print precision_score(y_test, pred)\n",
    "    print recall_score(y_test, pred)\n",
    "    metrics_test['Naive']['precision'].append(precision_score(y_test, pred))\n",
    "    metrics_test['Naive']['recall'].append(recall_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive: recall = 0.800000, precision = 0.140453\n",
      "SVM: recall = 0.066667, precision = 0.166667\n",
      "Forest: recall = 0.133333, precision = 0.500000\n"
     ]
    }
   ],
   "source": [
    "print 'Naive: recall = %f, precision = %f' %(np.mean(metrics_test['Naive']['recall']), np.mean(metrics_test['Naive']['precision']))\n",
    "print 'SVM: recall = %f, precision = %f' %(np.mean(metrics_test['SVM']['recall']), np.mean(metrics_test['SVM']['precision']))\n",
    "print 'Forest: recall = %f, precision = %f' %(np.mean(metrics_test['Forest']['recall']), np.mean(metrics_test['Forest']['precision']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(df[temp_features], df['poi'], test_size=0.3, random_state=42)\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "naive = GaussianNB()\n",
    "\n",
    "naive = naive.fit(features_train, labels_train)\n",
    "\n",
    "pred = naive.predict(features_test)\n",
    "\n",
    "print precision_score(labels_test, pred)\n",
    "print recall_score(labels_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Set the parameters by cross-validation\n",
    "### This is from http://scikit-learn.org/0.15/auto_examples/grid_search_digits.html\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=5, scoring=score)\n",
    "    clf.fit(features_train, labels_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_estimator_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    for params, mean_score, scores in clf.grid_scores_:\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean_score, scores.std() / 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = labels_test, clf.predict(features_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'n_estimators': [10, 100, 1000], 'min_samples_split': [5, 10, 20]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(RandomForestClassifier(), tuned_parameters, cv=5, scoring=score)\n",
    "    clf.fit(features_train, labels_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_estimator_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    for params, mean_score, scores in clf.grid_scores_:\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean_score, scores.std() / 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = labels_test, clf.predict(features_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Make features list for data dump\n",
    "features_list = ['poi'] + temp_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Reset index to names\n",
    "df = df.set_index(['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = df.to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
